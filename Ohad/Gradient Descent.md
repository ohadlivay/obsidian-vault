### learn weights
Repeat:
$w:= w-\alpha\frac{dJ(W)}{dW}$
where $w$ is the weights we learn
$\alpha$ is the [[learning rate]]
and $\frac{dJ(W)}{dW}$ is the deriviative, thus we move faster the further we are from the [[Global Minimum]] (given that the problem is a [[Convex Optimization]] problem) and we convege on the [[Global Minimum]].


