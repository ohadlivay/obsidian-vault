### learn weights
Repeat:
$w:= w-\alpha\frac{dJ(W)}{dW}$
where $w$ is the weights we learn
$\alpha$ is the [[learning rate]]
and $\frac{dJ(W)}{dW}$ is the deriviative, thus we move faster the further we are from the [[global minimum]] (given that the problem is a [[convex optimization]] problem) and we convege on the [[global minimum]].




#mytodo/empty
- [ ] complete this